{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9CB772hVR-9"
   },
   "source": [
    "# BERT Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "Hsm54Yi8VV_j",
    "outputId": "7e45a133-8bdd-4614-9ef0-f1760798fed3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-a89a7537-5a6c-4fa6-bc0e-567612eb83ff\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-a89a7537-5a6c-4fa6-bc0e-567612eb83ff\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train.tsv to train (1).tsv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "GNdQIpduVR_E",
    "outputId": "467a02f5-4208-42a1-dceb-ae4c5a750791"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>What are some special cares for someone with a...</td>\n",
       "      <td>How can I keep my nose from getting stuffy at ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>Is Career Launcher good for RBI Grade B prepar...</td>\n",
       "      <td>How is career launcher online program for RBI ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>Is Career Launcher good for RBI Grade B prepar...</td>\n",
       "      <td>How is career launcher online program for RBI ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "      <td>Is it normal to have a dark ring around the ir...</td>\n",
       "      <td>What causes a dark ring around the iris? How s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>What are the best associate product manager (A...</td>\n",
       "      <td>What are the general requirement to become a P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid1  qid2  ...                                          question2 is_duplicate\n",
       "id              ...                                                                \n",
       "31    63    64  ...  How can I keep my nose from getting stuffy at ...            1\n",
       "50   101   102  ...  How is career launcher online program for RBI ...            1\n",
       "50   101   102  ...  How is career launcher online program for RBI ...            1\n",
       "61   123   124  ...  What causes a dark ring around the iris? How s...            0\n",
       "98   197   198  ...  What are the general requirement to become a P...            0\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "dataset = pandas.read_csv('train.tsv', delimiter = \"\\t\", index_col=\"id\", nrows=5000)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "HyXJ8Hx8iS4G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqSVJoW9VR_F"
   },
   "source": [
    "Let's see what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnUW3i6wVR_F",
    "outputId": "d85dc30f-c896-46b8-b3c0-574d5974fb34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5000 entries, 31 to 333474\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   qid1          5000 non-null   int64 \n",
      " 1   qid2          5000 non-null   int64 \n",
      " 2   question1     5000 non-null   object\n",
      " 3   question2     5000 non-null   object\n",
      " 4   is_duplicate  5000 non-null   int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 234.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXcRQfXAVR_F"
   },
   "source": [
    "# Tokenizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ycg40lGWsc6",
    "outputId": "dc0e6ec0-0192-48c5-ed16-c15667e133b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers~=2.11.0 in /usr/local/lib/python3.7/dist-packages (2.11.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers~=2.11.0) (2.23.0)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.7/dist-packages (from transformers~=2.11.0) (0.7.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers~=2.11.0) (1.19.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers~=2.11.0) (0.1.95)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers~=2.11.0) (2019.12.20)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers~=2.11.0) (0.0.45)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers~=2.11.0) (20.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers~=2.11.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers~=2.11.0) (4.41.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=2.11.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=2.11.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=2.11.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=2.11.0) (2.10)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers~=2.11.0) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers~=2.11.0) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers~=2.11.0) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers~=2.11.0) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "!pip install transformers~=2.11.0\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "FrzsuzL2VR_G"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcwCxlusVR_G"
   },
   "source": [
    "Split a sentence into tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bJ3DhP6VR_H",
    "outputId": "5f99703f-0c15-485e-b08f-56fef746cac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: What are some special cares for someone with a nose that gets stuffy during the night?\n",
      "Tokenized: ['what', 'are', 'some', 'special', 'cares', 'for', 'someone', 'with', 'a', 'nose', 'that', 'gets', 'stuff', '##y', 'during', 'the', 'night', '?']\n",
      "Token IDs: [2054, 2024, 2070, 2569, 14977, 2005, 2619, 2007, 1037, 4451, 2008, 4152, 4933, 2100, 2076, 1996, 2305, 1029]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {dataset['question1'].iloc[0]}\")\n",
    "print(f\"Tokenized: {tokenizer.tokenize(dataset['question1'].iloc[0])}\")\n",
    "print(f\"Token IDs: {tokenizer.convert_tokens_to_ids(tokenizer.tokenize(dataset['question1'].iloc[0]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQlPW_6IVR_H"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1SAbDYgVR_H",
    "outputId": "f920f0e0-e1d2-4420-d81d-8db6ad074e70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: How can I keep my nose from getting stuffy at night?\n",
      "Tokenized: [101, 2129, 2064, 1045, 2562, 2026, 4451, 2013, 2893, 4933, 2100, 2012, 2305, 1029, 102]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {dataset['question2'].iloc[0]}\")\n",
    "print(f\"Tokenized: {tokenizer.encode(dataset['question2'].iloc[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9d-sZwnVR_H"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "SvIj9LqhVR_I",
    "outputId": "b42611e2-18c3-4dac-8dc6-fa48c4e2d6ec"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] what is the step by step guide to invest in share market? [SEP]'"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 2054, 2003, 1996, 3357, 2011, 3357, 5009, 2000, 15697, 1999, 3745, 3006, 1029, 102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84AODfaAVR_I"
   },
   "source": [
    "## Special Tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "AmTV5DutVR_I",
    "outputId": "aa10f8e5-6bd8-4078-ad51-e95ab41327e7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] what are some special cares for someone with a nose that gets stuffy during the night? [SEP] how can i keep my nose from getting stuffy at night? [SEP]'"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_pair = tokenizer.encode(dataset['question1'].iloc[0], dataset['question2'].iloc[0])\n",
    "tokenizer.decode(encoded_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0ym6SLLVR_J"
   },
   "source": [
    "## Sentence Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZ-CIiDQVR_J",
    "outputId": "f313ae41-7108-433e-8b5c-8ce53880fd93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "\n",
      "\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 1149/5000 [00:00<00:00, 11487.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 2839/5000 [00:00<00:00, 12706.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 14103.49it/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 2151/5000 [00:00<00:00, 21503.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 20012.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "dataset[\"question1_length\"] = dataset[\"question1\"].progress_apply(lambda question: \n",
    "                                                                                      len(tokenizer.tokenize(question)))\n",
    "dataset[\"question2_length\"] = dataset[\"question2\"].progress_apply(lambda question: \n",
    "                                                                                      len(tokenizer.tokenize(question)))\n",
    "dataset[\"joint_length\"] = dataset[\"question1_length\"] + dataset[\"question2_length\"]\n",
    "dataset[\"joint_length\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPtsTzjvVR_J"
   },
   "source": [
    "\n",
    "# Training & Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "46FIXLRKVR_K",
    "outputId": "dcb4c72a-48b6-4790-c045-269988dfcf61"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224450</th>\n",
       "      <td>What is the scope of medical microbiology?</td>\n",
       "      <td>Does medical microbiology have a good scope?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132744</th>\n",
       "      <td>What is best treatment of pancreatitis?</td>\n",
       "      <td>What is the best treatment of diabetes?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242425</th>\n",
       "      <td>Which medical insurance company has the best r...</td>\n",
       "      <td>Should I include my research abstract in my co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169829</th>\n",
       "      <td>Why do people think about \"what others will th...</td>\n",
       "      <td>Why do we care for others' opinion and about w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138281</th>\n",
       "      <td>How should one remove old blade cut marks from...</td>\n",
       "      <td>How do I remove old blade cut marks from hand ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1                                          question2\n",
       "id                                                                                                          \n",
       "224450         What is the scope of medical microbiology?       Does medical microbiology have a good scope?\n",
       "132744            What is best treatment of pancreatitis?            What is the best treatment of diabetes?\n",
       "242425  Which medical insurance company has the best r...  Should I include my research abstract in my co...\n",
       "169829  Why do people think about \"what others will th...  Why do we care for others' opinion and about w...\n",
       "138281  How should one remove old blade cut marks from...  How do I remove old blade cut marks from hand ..."
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[[\"question1\", \"question2\"]], \n",
    "                                                    dataset[\"is_duplicate\"], test_size=0.2, random_state=42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_utloYhVR_K",
    "outputId": "24b99168-003f-4e13-c39c-edc0c8204869"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "224450    0\n",
       "132744    0\n",
       "242425    0\n",
       "169829    1\n",
       "138281    1\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ik7FOUX2VR_K"
   },
   "source": [
    "# Tokenize Dataset and Create Dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJoJ4mwYVR_L",
    "outputId": "7db7608b-97b6-4c9c-8469-03878b436229"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  2003,  1996,  9531,  1997,  2966, 12702, 21685,  1029,\n",
       "           102,  2515,  2966, 12702, 21685,  2031,  1037,  2204,  9531,  1029,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 310\n",
    "tokenizer.encode_plus(X_train.iloc[0][\"question1\"], X_train.iloc[0][\"question2\"], max_length=max_length, \n",
    "                      pad_to_max_length=True, return_attention_mask=True, return_tensors='pt', truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNY4gLTWVR_L"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Qica82jKVR_L"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_dataset_torch(data: pandas.DataFrame, labels: pandas.Series) -> TensorDataset:\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        encoded_dict = tokenizer.encode_plus(row[\"question1\"], row[\"question2\"], max_length=max_length, pad_to_max_length=True, \n",
    "                      return_attention_mask=True, return_tensors='pt', truncation=True)\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        token_type_ids.append(encoded_dict[\"token_type_ids\"])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels.values)\n",
    "    \n",
    "    return TensorDataset(input_ids, attention_masks, token_type_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYzKVVOTVR_M",
    "outputId": "ce914f7c-a5aa-4528-bbb0-12d935ee0d60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3200 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 107/3200 [00:00<00:02, 1067.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 234/3200 [00:00<00:02, 1120.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 390/3200 [00:00<00:02, 1222.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 534/3200 [00:00<00:02, 1279.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 677/3200 [00:00<00:01, 1320.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 843/3200 [00:00<00:01, 1405.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 1012/3200 [00:00<00:01, 1479.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 1177/3200 [00:00<00:01, 1526.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████▏     | 1327/3200 [00:00<00:01, 1517.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 1477/3200 [00:01<00:01, 1451.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 1622/3200 [00:01<00:01, 1406.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 1763/3200 [00:01<00:01, 1406.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 1915/3200 [00:01<00:00, 1437.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 2077/3200 [00:01<00:00, 1487.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 2227/3200 [00:01<00:00, 1486.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 2376/3200 [00:01<00:00, 1423.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 2520/3200 [00:01<00:00, 1417.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 2663/3200 [00:03<00:01, 299.51it/s] \u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 2808/3200 [00:03<00:00, 392.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 2962/3200 [00:03<00:00, 505.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3200/3200 [00:03<00:00, 923.69it/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/800 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 157/800 [00:00<00:00, 1569.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 305/800 [00:00<00:00, 1539.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 467/800 [00:00<00:00, 1562.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 627/800 [00:00<00:00, 1572.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 800/800 [00:00<00:00, 1545.35it/s]\n"
     ]
    }
   ],
   "source": [
    "train = convert_to_dataset_torch(X_train, y_train)\n",
    "validation = convert_to_dataset_torch(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOYAhiadVR_M"
   },
   "source": [
    "We’ll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "BR4jJKRtVR_M"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "core_number = multiprocessing.cpu_count()\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train,  # The training samples.\n",
    "            sampler = RandomSampler(train), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size.\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            validation, # The validation samples.\n",
    "            sampler = SequentialSampler(validation), # Pull out batches sequentially.\n",
    "            batch_size = batch_size, # Evaluate with this batch size.\n",
    "\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G0IT7tIVR_N"
   },
   "source": [
    "# Trainig the Classification Model\n",
    "\n",
    "## BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dseTMzrRVR_N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12035_\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\12035_\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\12035_\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb96bac664f461fac20d4dca5647ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628c73e368a24b218f4ee4d9c7c1f2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels=2, # The number of output labels--2 for binary classification.\n",
    "  \n",
    "    output_attentions=False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states=False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q01x5y94VR_N"
   },
   "source": [
    "# Optimizer & Learning Rate Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "mIiCZNAWVR_O"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "\n",
    "adamw_optimizer = AdamW(bert_model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_xDaXIvVR_O"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "ePF4arMLVR_O"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(adamw_optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZJsE8-2VR_O"
   },
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "AFFo74V_VR_O"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zxna2GOJVR_P"
   },
   "source": [
    "This method is to train one batch over our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "uju9pJRHVR_P"
   },
   "outputs": [],
   "source": [
    "def fit_batch(dataloader, model, optimizer, epoch):\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=f\"Training epoch:{epoch}\", unit=\"batch\"):\n",
    "        input_ids, attention_masks, token_type_ids, labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        loss, _ = model(input_ids, \n",
    "                             token_type_ids=token_type_ids, \n",
    "                             attention_mask=attention_masks, \n",
    "                             labels=labels)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    return total_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5I-goSGRVR_P"
   },
   "source": [
    "This method is evalute one batch over our model. We're going to use scikit learn [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "K7sF_rvYVR_P"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_batch(dataloader, model, metric=accuracy_score):\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    predictions , predicted_labels = [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "        # Unpack batch from dataloader.\n",
    "        input_ids, attention_masks, token_type_ids, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(input_ids, \n",
    "                                   token_type_ids=token_type_ids, \n",
    "                                   attention_mask=attention_masks,\n",
    "                                   labels=labels)\n",
    "            #output = model(input_ids, token_type_ids=token_type_ids,attention_mask=attention_masks,labels=labels)\n",
    "            #loss = output.loss\n",
    "            #logits = output.logits\n",
    "\n",
    "\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        y_pred = numpy.argmax(logits.detach().numpy(), axis=1).flatten()\n",
    "        total_eval_accuracy += metric(labels, y_pred)\n",
    "        \n",
    "        predictions.extend(logits.detach().numpy().tolist())\n",
    "        predicted_labels.extend(y_pred.tolist())\n",
    "    \n",
    "    return total_eval_accuracy, total_eval_loss, predictions ,predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blXnH7_FVR_P"
   },
   "source": [
    "Main loop method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "V-kn696fVR_Q"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "numpy.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "\n",
    "\n",
    "def train(train_dataloader, validation_dataloader, model, optimizer, epochs):\n",
    "    training_stats = []\n",
    "    \n",
    "    total_t0 = time.time()\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        total_train_loss = fit_batch(train_dataloader, model, optimizer, epoch)\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        total_eval_accuracy, total_eval_loss, _, _ = eval_batch(validation_dataloader, model)\n",
    "        \n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        \n",
    "        print(f\"  Accuracy: {avg_val_accuracy}\")\n",
    "    \n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "        validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "        print(f\"  Validation Loss: {avg_val_loss}\")\n",
    "    \n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(f\"Total training took {format_time(time.time()-total_t0)}\")\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0cei-hRVR_Q"
   },
   "source": [
    "We’re ready to kick off the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pmz7KXpOVR_Q",
    "outputId": "a8e9281b-ea9e-42cd-e118-7267a00617cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training epoch:0:   0%|          | 0/200 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   0%|          | 1/200 [00:53<2:58:12, 53.73s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   1%|          | 2/200 [01:44<2:54:41, 52.94s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   2%|▏         | 3/200 [02:36<2:52:26, 52.52s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   2%|▏         | 4/200 [03:27<2:49:51, 52.00s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   2%|▎         | 5/200 [04:18<2:47:57, 51.68s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   3%|▎         | 6/200 [05:08<2:46:08, 51.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   4%|▎         | 7/200 [05:59<2:45:02, 51.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   4%|▍         | 8/200 [06:50<2:43:43, 51.17s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   4%|▍         | 9/200 [07:41<2:42:55, 51.18s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   5%|▌         | 10/200 [08:32<2:41:45, 51.08s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   6%|▌         | 11/200 [09:28<2:44:52, 52.34s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   6%|▌         | 12/200 [10:20<2:43:53, 52.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   6%|▋         | 13/200 [11:10<2:41:24, 51.79s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   7%|▋         | 14/200 [12:01<2:39:17, 51.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   8%|▊         | 15/200 [12:52<2:37:48, 51.18s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   8%|▊         | 16/200 [13:42<2:36:06, 50.90s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   8%|▊         | 17/200 [14:32<2:34:42, 50.72s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:   9%|▉         | 18/200 [15:22<2:33:27, 50.59s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  10%|▉         | 19/200 [16:13<2:32:29, 50.55s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  10%|█         | 20/200 [17:03<2:31:24, 50.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  10%|█         | 21/200 [17:53<2:30:25, 50.42s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  11%|█         | 22/200 [18:44<2:29:58, 50.55s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  12%|█▏        | 23/200 [19:38<2:31:43, 51.43s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  12%|█▏        | 24/200 [20:31<2:32:31, 52.00s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  12%|█▎        | 25/200 [21:22<2:30:33, 51.62s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  13%|█▎        | 26/200 [22:12<2:28:51, 51.33s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  14%|█▎        | 27/200 [23:03<2:27:33, 51.18s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  14%|█▍        | 28/200 [23:54<2:26:14, 51.01s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  14%|█▍        | 29/200 [24:45<2:25:08, 50.93s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  15%|█▌        | 30/200 [25:35<2:24:07, 50.87s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  16%|█▌        | 31/200 [26:26<2:23:07, 50.81s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  16%|█▌        | 32/200 [27:17<2:22:07, 50.76s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  16%|█▋        | 33/200 [28:07<2:21:03, 50.68s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  17%|█▋        | 34/200 [28:58<2:20:04, 50.63s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  18%|█▊        | 35/200 [29:56<2:25:26, 52.89s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  18%|█▊        | 36/200 [30:50<2:25:25, 53.20s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  18%|█▊        | 37/200 [31:40<2:22:23, 52.41s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  19%|█▉        | 38/200 [32:31<2:20:09, 51.91s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  20%|█▉        | 39/200 [33:22<2:18:22, 51.57s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  20%|██        | 40/200 [34:13<2:16:45, 51.29s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  20%|██        | 41/200 [35:03<2:15:20, 51.07s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  21%|██        | 42/200 [35:54<2:14:06, 50.93s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  22%|██▏       | 43/200 [36:44<2:12:59, 50.82s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  22%|██▏       | 44/200 [37:35<2:11:46, 50.68s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  22%|██▎       | 45/200 [38:25<2:10:44, 50.61s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  23%|██▎       | 46/200 [39:15<2:09:44, 50.55s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  24%|██▎       | 47/200 [40:12<2:13:35, 52.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  24%|██▍       | 48/200 [41:06<2:13:55, 52.86s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  24%|██▍       | 49/200 [41:57<2:11:15, 52.16s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  25%|██▌       | 50/200 [42:47<2:09:07, 51.65s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  26%|██▌       | 51/200 [43:37<2:07:15, 51.24s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  26%|██▌       | 52/200 [44:28<2:05:48, 51.00s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  26%|██▋       | 53/200 [45:18<2:04:36, 50.86s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  27%|██▋       | 54/200 [46:09<2:03:30, 50.76s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  28%|██▊       | 55/200 [46:59<2:02:24, 50.65s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  28%|██▊       | 56/200 [47:49<2:01:15, 50.53s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  28%|██▊       | 57/200 [48:40<2:00:22, 50.51s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  29%|██▉       | 58/200 [49:30<1:59:16, 50.40s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  30%|██▉       | 59/200 [50:23<2:00:19, 51.20s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  30%|███       | 60/200 [51:20<2:03:39, 52.99s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  30%|███       | 61/200 [52:11<2:01:17, 52.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  31%|███       | 62/200 [53:02<1:59:13, 51.84s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  32%|███▏      | 63/200 [53:52<1:57:17, 51.37s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  32%|███▏      | 64/200 [54:43<1:55:49, 51.10s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  32%|███▎      | 65/200 [55:33<1:54:29, 50.88s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  33%|███▎      | 66/200 [56:24<1:53:30, 50.82s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  34%|███▎      | 67/200 [57:14<1:52:24, 50.71s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  34%|███▍      | 68/200 [58:05<1:51:22, 50.62s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  34%|███▍      | 69/200 [58:55<1:50:25, 50.58s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  35%|███▌      | 70/200 [59:45<1:49:21, 50.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  36%|███▌      | 71/200 [1:00:36<1:48:31, 50.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  36%|███▌      | 72/200 [1:01:36<1:53:58, 53.43s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  36%|███▋      | 73/200 [1:02:27<1:51:17, 52.58s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  37%|███▋      | 74/200 [1:03:17<1:49:06, 51.95s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  38%|███▊      | 75/200 [1:04:07<1:47:14, 51.48s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  38%|███▊      | 76/200 [1:04:58<1:45:43, 51.16s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  38%|███▊      | 77/200 [1:05:48<1:44:24, 50.93s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  39%|███▉      | 78/200 [1:06:39<1:43:13, 50.77s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  40%|███▉      | 79/200 [1:07:29<1:42:02, 50.60s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  40%|████      | 80/200 [1:08:19<1:40:59, 50.49s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  40%|████      | 81/200 [1:09:10<1:40:07, 50.49s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  41%|████      | 82/200 [1:10:00<1:39:10, 50.43s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  42%|████▏     | 83/200 [1:10:50<1:38:22, 50.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  42%|████▏     | 84/200 [1:11:49<1:42:07, 52.82s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  42%|████▎     | 85/200 [1:12:39<1:40:01, 52.18s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  43%|████▎     | 86/200 [1:13:30<1:38:06, 51.64s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  44%|████▎     | 87/200 [1:14:20<1:36:32, 51.26s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  44%|████▍     | 88/200 [1:15:11<1:35:18, 51.06s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  44%|████▍     | 89/200 [1:16:01<1:34:08, 50.89s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  45%|████▌     | 90/200 [1:16:52<1:33:00, 50.73s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  46%|████▌     | 91/200 [1:17:42<1:32:06, 50.70s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  46%|████▌     | 92/200 [1:18:33<1:31:20, 50.74s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  46%|████▋     | 93/200 [1:19:24<1:30:37, 50.82s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  47%|████▋     | 94/200 [1:20:15<1:29:45, 50.81s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  48%|████▊     | 95/200 [1:21:06<1:28:51, 50.78s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  48%|████▊     | 96/200 [1:22:02<1:30:44, 52.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  48%|████▊     | 97/200 [1:22:52<1:29:04, 51.89s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  49%|████▉     | 98/200 [1:23:43<1:27:24, 51.41s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  50%|████▉     | 99/200 [1:24:33<1:26:04, 51.14s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  50%|█████     | 100/200 [1:25:24<1:24:48, 50.89s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  50%|█████     | 101/200 [1:26:14<1:23:47, 50.79s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  51%|█████     | 102/200 [1:27:05<1:22:48, 50.69s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  52%|█████▏    | 103/200 [1:27:55<1:21:55, 50.67s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  52%|█████▏    | 104/200 [1:28:46<1:21:00, 50.63s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  52%|█████▎    | 105/200 [1:29:36<1:20:05, 50.58s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  53%|█████▎    | 106/200 [1:30:27<1:19:12, 50.56s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  54%|█████▎    | 107/200 [1:31:17<1:18:19, 50.53s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  54%|█████▍    | 108/200 [1:32:17<1:21:34, 53.20s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  55%|█████▍    | 109/200 [1:33:07<1:19:32, 52.44s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  55%|█████▌    | 110/200 [1:33:58<1:17:49, 51.88s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  56%|█████▌    | 111/200 [1:34:48<1:16:20, 51.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  56%|█████▌    | 112/200 [1:35:39<1:15:02, 51.17s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  56%|█████▋    | 113/200 [1:36:29<1:13:53, 50.96s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  57%|█████▋    | 114/200 [1:37:20<1:12:47, 50.79s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  57%|█████▊    | 115/200 [1:38:10<1:11:49, 50.70s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  58%|█████▊    | 116/200 [1:39:01<1:10:55, 50.66s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  58%|█████▊    | 117/200 [1:39:51<1:09:54, 50.54s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  59%|█████▉    | 118/200 [1:40:42<1:09:03, 50.54s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  60%|█████▉    | 119/200 [1:41:32<1:08:07, 50.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  60%|██████    | 120/200 [1:42:30<1:10:23, 52.80s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  60%|██████    | 121/200 [1:43:21<1:08:35, 52.10s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  61%|██████    | 122/200 [1:44:11<1:07:04, 51.59s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  62%|██████▏   | 123/200 [1:45:01<1:05:44, 51.23s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  62%|██████▏   | 124/200 [1:45:52<1:04:33, 50.97s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  62%|██████▎   | 125/200 [1:46:42<1:03:29, 50.80s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  63%|██████▎   | 126/200 [1:47:32<1:02:28, 50.65s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  64%|██████▎   | 127/200 [1:48:23<1:01:36, 50.63s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  64%|██████▍   | 128/200 [1:49:13<1:00:41, 50.58s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  64%|██████▍   | 129/200 [1:50:04<59:51, 50.59s/batch]  \u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  65%|██████▌   | 130/200 [1:50:55<59:02, 50.60s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  66%|██████▌   | 131/200 [1:51:45<58:07, 50.55s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  66%|██████▌   | 132/200 [1:52:46<1:00:54, 53.74s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  66%|██████▋   | 133/200 [1:53:43<1:00:55, 54.56s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  67%|██████▋   | 134/200 [1:54:33<58:40, 53.34s/batch]  \u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  68%|██████▊   | 135/200 [1:55:24<56:50, 52.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  68%|██████▊   | 136/200 [1:56:14<55:21, 51.90s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  68%|██████▊   | 137/200 [1:57:05<54:06, 51.54s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  69%|██████▉   | 138/200 [1:57:56<52:57, 51.24s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  70%|██████▉   | 139/200 [1:58:46<51:57, 51.10s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  70%|███████   | 140/200 [1:59:37<50:57, 50.95s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  70%|███████   | 141/200 [2:00:28<50:10, 51.02s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  71%|███████   | 142/200 [2:01:19<49:12, 50.90s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  72%|███████▏  | 143/200 [2:02:09<48:13, 50.76s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  72%|███████▏  | 144/200 [2:03:02<47:53, 51.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  72%|███████▎  | 145/200 [2:04:02<49:29, 54.00s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  73%|███████▎  | 146/200 [2:04:53<47:41, 52.99s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  74%|███████▎  | 147/200 [2:05:43<46:13, 52.34s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  74%|███████▍  | 148/200 [2:06:34<44:54, 51.83s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  74%|███████▍  | 149/200 [2:07:25<43:42, 51.41s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  75%|███████▌  | 150/200 [2:08:15<42:39, 51.19s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  76%|███████▌  | 151/200 [2:09:06<41:38, 51.00s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  76%|███████▌  | 152/200 [2:09:56<40:40, 50.84s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  76%|███████▋  | 153/200 [2:10:47<39:47, 50.80s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  77%|███████▋  | 154/200 [2:11:37<38:53, 50.72s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  78%|███████▊  | 155/200 [2:12:28<38:01, 50.71s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  78%|███████▊  | 156/200 [2:13:19<37:09, 50.68s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  78%|███████▊  | 157/200 [2:14:22<38:57, 54.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  79%|███████▉  | 158/200 [2:15:12<37:16, 53.24s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  80%|███████▉  | 159/200 [2:16:03<35:53, 52.53s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  80%|████████  | 160/200 [2:16:54<34:40, 52.01s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  80%|████████  | 161/200 [2:17:45<33:34, 51.64s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  81%|████████  | 162/200 [2:18:36<32:35, 51.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  82%|████████▏ | 163/200 [2:19:27<31:38, 51.30s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  82%|████████▏ | 164/200 [2:20:18<30:43, 51.22s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  82%|████████▎ | 165/200 [2:21:09<29:49, 51.13s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  83%|████████▎ | 166/200 [2:22:00<28:57, 51.11s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  84%|████████▎ | 167/200 [2:22:51<28:03, 51.01s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  84%|████████▍ | 168/200 [2:23:41<27:08, 50.88s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  84%|████████▍ | 169/200 [2:24:42<27:51, 53.91s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  85%|████████▌ | 170/200 [2:25:33<26:32, 53.10s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  86%|████████▌ | 171/200 [2:26:24<25:22, 52.49s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  86%|████████▌ | 172/200 [2:27:16<24:21, 52.18s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  86%|████████▋ | 173/200 [2:28:07<23:18, 51.81s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  87%|████████▋ | 174/200 [2:28:58<22:20, 51.55s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  88%|████████▊ | 175/200 [2:29:49<21:23, 51.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  88%|████████▊ | 176/200 [2:30:40<20:29, 51.23s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  88%|████████▊ | 177/200 [2:31:30<19:34, 51.08s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  89%|████████▉ | 178/200 [2:32:22<18:45, 51.16s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  90%|████████▉ | 179/200 [2:33:13<17:53, 51.11s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  90%|█████████ | 180/200 [2:34:04<17:01, 51.05s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  90%|█████████ | 181/200 [2:35:05<17:06, 54.04s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  91%|█████████ | 182/200 [2:35:56<15:56, 53.14s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  92%|█████████▏| 183/200 [2:36:47<14:53, 52.57s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  92%|█████████▏| 184/200 [2:37:38<13:53, 52.12s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  92%|█████████▎| 185/200 [2:38:29<12:56, 51.80s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  93%|█████████▎| 186/200 [2:39:20<12:00, 51.49s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  94%|█████████▎| 187/200 [2:40:11<11:07, 51.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  94%|█████████▍| 188/200 [2:41:02<10:14, 51.23s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  94%|█████████▍| 189/200 [2:41:53<09:22, 51.13s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  95%|█████████▌| 190/200 [2:42:44<08:31, 51.16s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  96%|█████████▌| 191/200 [2:43:35<07:40, 51.19s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  96%|█████████▌| 192/200 [2:44:26<06:50, 51.26s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  96%|█████████▋| 193/200 [2:45:26<06:16, 53.74s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  97%|█████████▋| 194/200 [2:46:17<05:17, 52.95s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  98%|█████████▊| 195/200 [2:47:08<04:21, 52.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  98%|█████████▊| 196/200 [2:47:59<03:28, 52.06s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  98%|█████████▊| 197/200 [2:48:51<02:35, 51.90s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0:  99%|█████████▉| 198/200 [2:49:42<01:43, 51.72s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0: 100%|█████████▉| 199/200 [2:50:34<00:51, 51.70s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:0: 100%|██████████| 200/200 [2:51:25<00:00, 51.43s/batch]\n",
      "\n",
      "\n",
      "Evaluating:   0%|          | 0/50 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   2%|▏         | 1/50 [00:16<13:24, 16.41s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   4%|▍         | 2/50 [00:32<13:07, 16.41s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   6%|▌         | 3/50 [00:49<12:50, 16.40s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   8%|▊         | 4/50 [01:05<12:33, 16.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  10%|█         | 5/50 [01:21<12:16, 16.37s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  12%|█▏        | 6/50 [01:38<12:00, 16.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  14%|█▍        | 7/50 [01:54<11:45, 16.40s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  16%|█▌        | 8/50 [02:11<11:28, 16.40s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  18%|█▊        | 9/50 [02:27<11:17, 16.52s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  20%|██        | 10/50 [02:44<11:00, 16.52s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  22%|██▏       | 11/50 [03:00<10:44, 16.52s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  24%|██▍       | 12/50 [03:17<10:25, 16.46s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  26%|██▌       | 13/50 [03:35<10:27, 16.97s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  28%|██▊       | 14/50 [03:56<10:56, 18.23s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  30%|███       | 15/50 [04:15<10:40, 18.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  32%|███▏      | 16/50 [04:31<10:03, 17.74s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  34%|███▍      | 17/50 [04:47<09:31, 17.33s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  36%|███▌      | 18/50 [05:04<09:04, 17.02s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  38%|███▊      | 19/50 [05:20<08:40, 16.81s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  40%|████      | 20/50 [05:36<08:20, 16.67s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  42%|████▏     | 21/50 [05:53<08:01, 16.60s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|████▍     | 22/50 [06:09<07:42, 16.52s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  46%|████▌     | 23/50 [06:26<07:24, 16.48s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  48%|████▊     | 24/50 [06:42<07:08, 16.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  50%|█████     | 25/50 [06:58<06:50, 16.43s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  52%|█████▏    | 26/50 [07:15<06:33, 16.38s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  54%|█████▍    | 27/50 [07:31<06:15, 16.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████▌    | 28/50 [07:47<06:00, 16.37s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  58%|█████▊    | 29/50 [08:04<05:43, 16.37s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  60%|██████    | 30/50 [08:20<05:27, 16.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  62%|██████▏   | 31/50 [08:36<05:11, 16.37s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  64%|██████▍   | 32/50 [08:53<04:55, 16.41s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  66%|██████▌   | 33/50 [09:09<04:39, 16.44s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  68%|██████▊   | 34/50 [09:26<04:23, 16.44s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  70%|███████   | 35/50 [09:42<04:06, 16.44s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  72%|███████▏  | 36/50 [09:59<03:49, 16.40s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  74%|███████▍  | 37/50 [10:15<03:32, 16.37s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  76%|███████▌  | 38/50 [10:31<03:16, 16.34s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  78%|███████▊  | 39/50 [10:47<02:59, 16.32s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  80%|████████  | 40/50 [11:04<02:43, 16.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  82%|████████▏ | 41/50 [11:20<02:26, 16.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  84%|████████▍ | 42/50 [11:36<02:10, 16.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  86%|████████▌ | 43/50 [11:53<01:54, 16.30s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  88%|████████▊ | 44/50 [12:09<01:37, 16.32s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  90%|█████████ | 45/50 [12:25<01:21, 16.33s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  92%|█████████▏| 46/50 [12:42<01:05, 16.33s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  94%|█████████▍| 47/50 [12:58<00:49, 16.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  96%|█████████▌| 48/50 [13:14<00:32, 16.36s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  98%|█████████▊| 49/50 [13:31<00:16, 16.34s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|██████████| 50/50 [13:47<00:00, 16.55s/batch]\n",
      "\n",
      "\n",
      "Training epoch:1:   0%|          | 0/200 [00:00<?, ?batch/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.895\n",
      "  Validation Loss: 0.26888024136424066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training epoch:1:   0%|          | 1/200 [00:56<3:07:55, 56.66s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   1%|          | 2/200 [01:47<3:01:06, 54.88s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   2%|▏         | 3/200 [02:38<2:56:09, 53.65s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   2%|▏         | 4/200 [03:29<2:52:31, 52.82s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   2%|▎         | 5/200 [04:19<2:49:39, 52.20s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   3%|▎         | 6/200 [05:10<2:47:23, 51.77s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   4%|▎         | 7/200 [06:01<2:45:29, 51.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   4%|▍         | 8/200 [06:52<2:44:04, 51.27s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   4%|▍         | 9/200 [07:42<2:42:45, 51.13s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   5%|▌         | 10/200 [08:33<2:41:32, 51.01s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   6%|▌         | 11/200 [09:24<2:40:39, 51.00s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   6%|▌         | 12/200 [10:15<2:39:37, 50.94s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   6%|▋         | 13/200 [11:09<2:41:14, 51.74s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   7%|▋         | 14/200 [11:59<2:39:36, 51.49s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   8%|▊         | 15/200 [12:50<2:38:13, 51.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   8%|▊         | 16/200 [13:41<2:37:02, 51.21s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   8%|▊         | 17/200 [14:32<2:35:53, 51.11s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:   9%|▉         | 18/200 [15:23<2:35:05, 51.13s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  10%|▉         | 19/200 [16:14<2:33:50, 51.00s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  10%|█         | 20/200 [17:05<2:32:54, 50.97s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  10%|█         | 21/200 [17:56<2:31:56, 50.93s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  11%|█         | 22/200 [18:47<2:30:55, 50.87s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  12%|█▏        | 23/200 [19:38<2:30:09, 50.90s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  12%|█▏        | 24/200 [20:28<2:29:13, 50.87s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  12%|█▎        | 25/200 [21:22<2:31:11, 51.84s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  13%|█▎        | 26/200 [22:13<2:29:27, 51.54s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  14%|█▎        | 27/200 [23:04<2:27:53, 51.29s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  14%|█▍        | 28/200 [23:55<2:26:36, 51.14s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  14%|█▍        | 29/200 [24:45<2:25:24, 51.02s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  15%|█▌        | 30/200 [25:36<2:24:26, 50.98s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  16%|█▌        | 31/200 [26:27<2:23:23, 50.91s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  16%|█▌        | 32/200 [27:18<2:22:27, 50.88s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  16%|█▋        | 33/200 [28:09<2:21:25, 50.81s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  17%|█▋        | 34/200 [28:59<2:20:30, 50.79s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  18%|█▊        | 35/200 [29:50<2:19:41, 50.80s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  18%|█▊        | 36/200 [30:41<2:18:53, 50.82s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  18%|█▊        | 37/200 [31:42<2:26:26, 53.90s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  19%|█▉        | 38/200 [32:38<2:26:58, 54.44s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  20%|█▉        | 39/200 [33:29<2:23:06, 53.33s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  20%|██        | 40/200 [34:19<2:19:52, 52.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  20%|██        | 41/200 [35:09<2:17:29, 51.88s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  21%|██        | 42/200 [36:00<2:15:30, 51.46s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  22%|██▏       | 43/200 [36:51<2:13:59, 51.20s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  22%|██▏       | 44/200 [37:41<2:12:48, 51.08s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  22%|██▎       | 45/200 [38:32<2:11:32, 50.92s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  23%|██▎       | 46/200 [39:23<2:10:34, 50.87s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  24%|██▎       | 47/200 [40:13<2:09:27, 50.77s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  24%|██▍       | 48/200 [41:04<2:08:36, 50.77s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  24%|██▍       | 49/200 [41:57<2:09:09, 51.32s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  25%|██▌       | 50/200 [42:57<2:14:52, 53.95s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  26%|██▌       | 51/200 [43:47<2:11:31, 52.97s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  26%|██▌       | 52/200 [44:38<2:08:55, 52.26s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  26%|██▋       | 53/200 [45:29<2:07:04, 51.87s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  27%|██▋       | 54/200 [46:19<2:05:14, 51.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  28%|██▊       | 55/200 [47:10<2:03:54, 51.27s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  28%|██▊       | 56/200 [48:01<2:02:44, 51.15s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  28%|██▊       | 57/200 [48:52<2:01:38, 51.04s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  29%|██▉       | 58/200 [49:43<2:00:38, 50.97s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  30%|██▉       | 59/200 [50:34<1:59:42, 50.94s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  30%|███       | 60/200 [51:24<1:58:45, 50.90s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  30%|███       | 61/200 [52:15<1:57:52, 50.88s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  31%|███       | 62/200 [53:15<2:03:04, 53.51s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  32%|███▏      | 63/200 [54:06<2:00:21, 52.71s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  32%|███▏      | 64/200 [54:57<1:58:15, 52.17s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  32%|███▎      | 65/200 [55:47<1:56:29, 51.78s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  33%|███▎      | 66/200 [56:38<1:54:56, 51.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  34%|███▎      | 67/200 [57:29<1:53:45, 51.32s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  34%|███▍      | 68/200 [58:20<1:52:28, 51.13s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  34%|███▍      | 69/200 [59:11<1:51:25, 51.04s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  35%|███▌      | 70/200 [1:00:01<1:50:25, 50.96s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  36%|███▌      | 71/200 [1:00:53<1:49:39, 51.01s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  36%|███▌      | 72/200 [1:01:43<1:48:42, 50.96s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  36%|███▋      | 73/200 [1:02:34<1:47:38, 50.86s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  37%|███▋      | 74/200 [1:03:35<1:53:07, 53.87s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  38%|███▊      | 75/200 [1:04:26<1:50:20, 52.96s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  38%|███▊      | 76/200 [1:05:17<1:48:10, 52.34s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  38%|███▊      | 77/200 [1:06:07<1:46:20, 51.87s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  39%|███▉      | 78/200 [1:06:58<1:44:49, 51.55s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  40%|███▉      | 79/200 [1:07:49<1:43:30, 51.33s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  40%|████      | 80/200 [1:08:40<1:42:19, 51.16s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  40%|████      | 81/200 [1:09:31<1:41:23, 51.12s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  41%|████      | 82/200 [1:10:22<1:40:15, 50.98s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  42%|████▏     | 83/200 [1:11:12<1:39:22, 50.97s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  42%|████▏     | 84/200 [1:12:03<1:38:23, 50.89s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  42%|████▎     | 85/200 [1:12:54<1:37:23, 50.81s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  43%|████▎     | 86/200 [1:13:52<1:40:51, 53.08s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  44%|████▎     | 87/200 [1:14:43<1:38:46, 52.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  44%|████▍     | 88/200 [1:15:34<1:37:08, 52.04s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  44%|████▍     | 89/200 [1:16:25<1:35:25, 51.58s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  45%|████▌     | 90/200 [1:17:16<1:34:13, 51.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  46%|████▌     | 91/200 [1:18:07<1:33:05, 51.24s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  46%|████▌     | 92/200 [1:18:57<1:32:00, 51.12s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  46%|████▋     | 93/200 [1:19:49<1:31:12, 51.15s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  47%|████▋     | 94/200 [1:20:40<1:30:13, 51.07s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  48%|████▊     | 95/200 [1:21:30<1:29:18, 51.04s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  48%|████▊     | 96/200 [1:22:21<1:28:21, 50.98s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  48%|████▊     | 97/200 [1:23:12<1:27:32, 51.00s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  49%|████▉     | 98/200 [1:24:13<1:31:24, 53.76s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  50%|████▉     | 99/200 [1:25:04<1:29:08, 52.96s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  50%|█████     | 100/200 [1:25:55<1:27:18, 52.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  50%|█████     | 101/200 [1:26:46<1:25:41, 51.94s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  51%|█████     | 102/200 [1:27:37<1:24:24, 51.68s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  52%|█████▏    | 103/200 [1:28:27<1:23:03, 51.38s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  52%|█████▏    | 104/200 [1:29:18<1:21:57, 51.23s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  52%|█████▎    | 105/200 [1:30:09<1:20:47, 51.03s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  53%|█████▎    | 106/200 [1:31:00<1:20:01, 51.08s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  54%|█████▎    | 107/200 [1:31:51<1:19:03, 51.01s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  54%|█████▍    | 108/200 [1:32:42<1:18:04, 50.92s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  55%|█████▍    | 109/200 [1:33:33<1:17:14, 50.93s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  55%|█████▌    | 110/200 [1:34:31<1:19:51, 53.24s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  56%|█████▌    | 111/200 [1:35:22<1:18:03, 52.62s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  56%|█████▌    | 112/200 [1:36:13<1:16:22, 52.08s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  56%|█████▋    | 113/200 [1:37:04<1:14:55, 51.67s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  57%|█████▋    | 114/200 [1:37:55<1:13:38, 51.37s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  57%|█████▊    | 115/200 [1:38:45<1:12:28, 51.16s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  58%|█████▊    | 116/200 [1:39:36<1:11:26, 51.03s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  58%|█████▊    | 117/200 [1:40:27<1:10:26, 50.92s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  59%|█████▉    | 118/200 [1:41:18<1:09:40, 50.98s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  60%|█████▉    | 119/200 [1:42:08<1:08:40, 50.87s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  60%|██████    | 120/200 [1:42:59<1:07:46, 50.83s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  60%|██████    | 121/200 [1:43:50<1:06:52, 50.79s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  61%|██████    | 122/200 [1:44:50<1:09:31, 53.49s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  62%|██████▏   | 123/200 [1:45:40<1:07:36, 52.68s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  62%|██████▏   | 124/200 [1:46:31<1:05:59, 52.10s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  62%|██████▎   | 125/200 [1:47:22<1:04:42, 51.77s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  63%|██████▎   | 126/200 [1:48:13<1:03:25, 51.43s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  64%|██████▎   | 127/200 [1:49:03<1:02:20, 51.24s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  64%|██████▍   | 128/200 [1:49:54<1:01:23, 51.16s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  64%|██████▍   | 129/200 [1:50:45<1:00:26, 51.08s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  65%|██████▌   | 130/200 [1:51:36<59:33, 51.06s/batch]  \u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  66%|██████▌   | 131/200 [1:52:27<58:40, 51.02s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  66%|██████▌   | 132/200 [1:53:18<57:44, 50.95s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  66%|██████▋   | 133/200 [1:54:09<56:43, 50.79s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  67%|██████▋   | 134/200 [1:55:06<58:11, 52.91s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  68%|██████▊   | 135/200 [1:55:57<56:35, 52.24s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  68%|██████▊   | 136/200 [1:56:48<55:20, 51.88s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  68%|██████▊   | 137/200 [1:57:39<54:16, 51.69s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  69%|██████▉   | 138/200 [1:58:30<53:08, 51.42s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  70%|██████▉   | 139/200 [1:59:21<52:10, 51.32s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  70%|███████   | 140/200 [2:00:12<51:10, 51.17s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  70%|███████   | 141/200 [2:01:03<50:15, 51.11s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  71%|███████   | 142/200 [2:01:54<49:20, 51.03s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  72%|███████▏  | 143/200 [2:02:45<48:27, 51.01s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  72%|███████▏  | 144/200 [2:03:36<47:32, 50.95s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  72%|███████▎  | 145/200 [2:04:26<46:36, 50.84s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  73%|███████▎  | 146/200 [2:05:28<48:39, 54.07s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  74%|███████▎  | 147/200 [2:06:19<46:53, 53.09s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  74%|███████▍  | 148/200 [2:07:09<45:24, 52.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  74%|███████▍  | 149/200 [2:08:00<44:07, 51.91s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  75%|███████▌  | 150/200 [2:08:51<42:59, 51.60s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  76%|███████▌  | 151/200 [2:09:42<41:59, 51.42s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  76%|███████▌  | 152/200 [2:10:33<41:03, 51.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  76%|███████▋  | 153/200 [2:11:24<40:07, 51.23s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  77%|███████▋  | 154/200 [2:12:15<39:11, 51.13s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  78%|███████▊  | 155/200 [2:13:06<38:19, 51.10s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  78%|███████▊  | 156/200 [2:13:57<37:27, 51.08s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  78%|███████▊  | 157/200 [2:14:48<36:35, 51.06s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  79%|███████▉  | 158/200 [2:15:46<37:15, 53.22s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  80%|███████▉  | 159/200 [2:16:37<35:53, 52.52s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  80%|████████  | 160/200 [2:17:28<34:44, 52.12s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  80%|████████  | 161/200 [2:18:19<33:37, 51.72s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  81%|████████  | 162/200 [2:19:10<32:39, 51.56s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  82%|████████▏ | 163/200 [2:20:01<31:40, 51.36s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  82%|████████▏ | 164/200 [2:20:53<30:48, 51.34s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  82%|████████▎ | 165/200 [2:21:44<29:55, 51.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  83%|████████▎ | 166/200 [2:22:35<29:02, 51.25s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  84%|████████▎ | 167/200 [2:23:26<28:11, 51.26s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  84%|████████▍ | 168/200 [2:24:18<27:21, 51.29s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  84%|████████▍ | 169/200 [2:25:09<26:35, 51.46s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  85%|████████▌ | 170/200 [2:26:10<27:02, 54.09s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  86%|████████▌ | 171/200 [2:27:01<25:43, 53.23s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  86%|████████▌ | 172/200 [2:27:52<24:33, 52.62s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  86%|████████▋ | 173/200 [2:28:43<23:26, 52.10s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  87%|████████▋ | 174/200 [2:29:34<22:26, 51.80s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  88%|████████▊ | 175/200 [2:30:25<21:28, 51.54s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  88%|████████▊ | 176/200 [2:31:16<20:33, 51.38s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  88%|████████▊ | 177/200 [2:32:07<19:39, 51.26s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  89%|████████▉ | 178/200 [2:32:58<18:44, 51.12s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  90%|████████▉ | 179/200 [2:33:49<17:51, 51.02s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  90%|█████████ | 180/200 [2:34:39<16:58, 50.94s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  90%|█████████ | 181/200 [2:35:31<16:09, 51.04s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  91%|█████████ | 182/200 [2:36:30<16:03, 53.51s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  92%|█████████▏| 183/200 [2:37:21<14:57, 52.78s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  92%|█████████▏| 184/200 [2:38:12<13:54, 52.18s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  92%|█████████▎| 185/200 [2:39:03<12:57, 51.83s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  93%|█████████▎| 186/200 [2:39:54<12:02, 51.62s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  94%|█████████▎| 187/200 [2:40:45<11:10, 51.56s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  94%|█████████▍| 188/200 [2:41:36<10:17, 51.43s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  94%|█████████▍| 189/200 [2:42:27<09:23, 51.26s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  95%|█████████▌| 190/200 [2:43:18<08:31, 51.19s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  96%|█████████▌| 191/200 [2:44:09<07:39, 51.08s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  96%|█████████▌| 192/200 [2:45:00<06:48, 51.02s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  96%|█████████▋| 193/200 [2:45:51<05:57, 51.11s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  97%|█████████▋| 194/200 [2:46:51<05:22, 53.73s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  98%|█████████▊| 195/200 [2:47:42<04:24, 52.90s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  98%|█████████▊| 196/200 [2:48:33<03:29, 52.25s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  98%|█████████▊| 197/200 [2:49:24<02:35, 51.87s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1:  99%|█████████▉| 198/200 [2:50:15<01:43, 51.53s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1: 100%|█████████▉| 199/200 [2:51:06<00:51, 51.43s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Training epoch:1: 100%|██████████| 200/200 [2:51:57<00:00, 51.59s/batch]\n",
      "\n",
      "\n",
      "Evaluating:   0%|          | 0/50 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   2%|▏         | 1/50 [00:16<13:22, 16.38s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   4%|▍         | 2/50 [00:32<13:06, 16.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   6%|▌         | 3/50 [00:49<12:51, 16.42s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   8%|▊         | 4/50 [01:05<12:35, 16.41s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  10%|█         | 5/50 [01:22<12:18, 16.42s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  12%|█▏        | 6/50 [01:38<12:01, 16.41s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  14%|█▍        | 7/50 [01:54<11:45, 16.40s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  16%|█▌        | 8/50 [02:11<11:28, 16.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  18%|█▊        | 9/50 [02:27<11:12, 16.40s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  20%|██        | 10/50 [02:44<10:57, 16.44s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  22%|██▏       | 11/50 [03:00<10:41, 16.44s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  24%|██▍       | 12/50 [03:17<10:25, 16.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  26%|██▌       | 13/50 [03:33<10:08, 16.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  28%|██▊       | 14/50 [03:50<09:52, 16.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  30%|███       | 15/50 [04:06<09:34, 16.42s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  32%|███▏      | 16/50 [04:27<10:07, 17.88s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  34%|███▍      | 17/50 [04:46<10:03, 18.28s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  36%|███▌      | 18/50 [05:03<09:27, 17.72s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  38%|███▊      | 19/50 [05:19<08:56, 17.32s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  40%|████      | 20/50 [05:36<08:31, 17.05s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  42%|████▏     | 21/50 [05:52<08:09, 16.87s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|████▍     | 22/50 [06:08<07:47, 16.71s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  46%|████▌     | 23/50 [06:25<07:28, 16.60s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  48%|████▊     | 24/50 [06:41<07:09, 16.53s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  50%|█████     | 25/50 [06:57<06:51, 16.48s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  52%|█████▏    | 26/50 [07:14<06:34, 16.44s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  54%|█████▍    | 27/50 [07:30<06:17, 16.41s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████▌    | 28/50 [07:47<06:01, 16.42s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  58%|█████▊    | 29/50 [08:03<05:44, 16.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  60%|██████    | 30/50 [08:19<05:27, 16.38s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  62%|██████▏   | 31/50 [08:36<05:11, 16.40s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  64%|██████▍   | 32/50 [08:52<04:54, 16.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  66%|██████▌   | 33/50 [09:08<04:38, 16.37s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  68%|██████▊   | 34/50 [09:25<04:21, 16.36s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  70%|███████   | 35/50 [09:41<04:05, 16.36s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  72%|███████▏  | 36/50 [09:57<03:49, 16.38s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  74%|███████▍  | 37/50 [10:14<03:32, 16.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  76%|███████▌  | 38/50 [10:30<03:16, 16.34s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  78%|███████▊  | 39/50 [10:46<02:59, 16.36s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  80%|████████  | 40/50 [11:03<02:43, 16.36s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  82%|████████▏ | 41/50 [11:19<02:27, 16.36s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  84%|████████▍ | 42/50 [11:36<02:10, 16.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  86%|████████▌ | 43/50 [11:52<01:54, 16.36s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  88%|████████▊ | 44/50 [12:08<01:38, 16.34s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  90%|█████████ | 45/50 [12:25<01:21, 16.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  92%|█████████▏| 46/50 [12:41<01:05, 16.36s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  94%|█████████▍| 47/50 [12:57<00:49, 16.37s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  96%|█████████▌| 48/50 [13:14<00:32, 16.36s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  98%|█████████▊| 49/50 [13:30<00:16, 16.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|██████████| 50/50 [13:46<00:00, 16.54s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.89625\n",
      "  Validation Loss: 0.2777453856170177\n",
      "\n",
      "Training complete!\n",
      "Total training took 6:10:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_stats = train(train_dataloader, validation_dataloader, bert_model, adamw_optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcvwnL4JVR_Q"
   },
   "source": [
    "Let’s view the summary of the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "EnQU8KytVR_Q",
    "outputId": "8a959a9d-1b9c-4e41-e35b-3005a45972d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.419501</td>\n",
       "      <td>0.268880</td>\n",
       "      <td>0.89500</td>\n",
       "      <td>2:51:25</td>\n",
       "      <td>0:13:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.194195</td>\n",
       "      <td>0.277745</td>\n",
       "      <td>0.89625</td>\n",
       "      <td>2:51:57</td>\n",
       "      <td>0:13:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "0           0.419501     0.268880        0.89500       2:51:25         0:13:48\n",
       "1           0.194195     0.277745        0.89625       2:51:57         0:13:47"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats = pandas.DataFrame(training_stats).set_index('epoch')\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "tOJrydaiVR_R",
    "outputId": "377defdc-75ec-47fa-97c4-16d6a3e58202"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "pyplot.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "pyplot.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "pyplot.title(\"Training & Validation Loss\")\n",
    "pyplot.xlabel(\"Epoch\")\n",
    "pyplot.ylabel(\"Loss\")\n",
    "pyplot.legend()\n",
    "pyplot.xticks(df_stats.index.values.tolist())\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFYKVbkcVR_R"
   },
   "source": [
    "# Performance On Test Set\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7bhW4g5VR_R",
    "outputId": "aa32493d-6e99-41cc-8296-8211891f0569"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 1/1000 [00:00<01:43,  9.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 84/1000 [00:00<01:06, 13.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 191/1000 [00:00<00:41, 19.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 304/1000 [00:00<00:25, 27.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 453/1000 [00:00<00:13, 39.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 603/1000 [00:00<00:07, 55.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 743/1000 [00:00<00:03, 77.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1129.58it/s]\n"
     ]
    }
   ],
   "source": [
    "test = convert_to_dataset_torch(X_test, y_test)\n",
    "test_dataloader = DataLoader(test,  sampler=SequentialSampler(test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpuuk0miVR_R"
   },
   "source": [
    "## Evaluate on Test Set\n",
    "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lh1FlsmkVR_R",
    "outputId": "812babe4-6b4d-49eb-a871-5d36d303831c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluating:   0%|          | 0/63 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   2%|▏         | 1/63 [00:16<16:57, 16.42s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   3%|▎         | 2/63 [00:32<16:39, 16.39s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   5%|▍         | 3/63 [00:49<16:21, 16.36s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   6%|▋         | 4/63 [01:09<17:12, 17.50s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   8%|▊         | 5/63 [01:29<17:49, 18.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  10%|▉         | 6/63 [01:46<17:07, 18.03s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  11%|█         | 7/63 [02:03<16:22, 17.55s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  13%|█▎        | 8/63 [02:19<15:45, 17.19s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  14%|█▍        | 9/63 [02:36<15:14, 16.93s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  16%|█▌        | 10/63 [02:52<14:46, 16.73s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  17%|█▋        | 11/63 [03:08<14:24, 16.62s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  19%|█▉        | 12/63 [03:24<14:02, 16.52s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  21%|██        | 13/63 [03:41<13:42, 16.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  22%|██▏       | 14/63 [03:57<13:24, 16.41s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  24%|██▍       | 15/63 [04:13<13:05, 16.37s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  25%|██▌       | 16/63 [04:30<12:49, 16.38s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  27%|██▋       | 17/63 [04:46<12:32, 16.35s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  29%|██▊       | 18/63 [05:02<12:14, 16.33s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  30%|███       | 19/63 [05:19<11:57, 16.30s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  32%|███▏      | 20/63 [05:35<11:40, 16.30s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|███▎      | 21/63 [05:51<11:25, 16.31s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  35%|███▍      | 22/63 [06:07<11:09, 16.32s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  37%|███▋      | 23/63 [06:24<10:52, 16.32s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  38%|███▊      | 24/63 [06:40<10:40, 16.43s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  40%|███▉      | 25/63 [06:57<10:24, 16.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  41%|████▏     | 26/63 [07:14<10:10, 16.49s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  43%|████▎     | 27/63 [07:30<09:54, 16.51s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|████▍     | 28/63 [07:46<09:36, 16.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  46%|████▌     | 29/63 [08:03<09:21, 16.50s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  48%|████▊     | 30/63 [08:20<09:05, 16.53s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  49%|████▉     | 31/63 [08:36<08:50, 16.57s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  51%|█████     | 32/63 [08:53<08:33, 16.57s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  52%|█████▏    | 33/63 [09:09<08:16, 16.53s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  54%|█████▍    | 34/63 [09:26<07:58, 16.51s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████▌    | 35/63 [09:42<07:41, 16.49s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  57%|█████▋    | 36/63 [09:59<07:24, 16.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  59%|█████▊    | 37/63 [10:15<07:08, 16.46s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  60%|██████    | 38/63 [10:32<06:53, 16.54s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  62%|██████▏   | 39/63 [10:48<06:35, 16.48s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  63%|██████▎   | 40/63 [11:05<06:19, 16.50s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  65%|██████▌   | 41/63 [11:21<06:02, 16.46s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|██████▋   | 42/63 [11:38<05:45, 16.46s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  68%|██████▊   | 43/63 [11:54<05:30, 16.51s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  70%|██████▉   | 44/63 [12:11<05:13, 16.51s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  71%|███████▏  | 45/63 [12:27<04:57, 16.51s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  73%|███████▎  | 46/63 [12:44<04:39, 16.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  75%|███████▍  | 47/63 [13:00<04:23, 16.48s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  76%|███████▌  | 48/63 [13:17<04:07, 16.48s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  78%|███████▊  | 49/63 [13:33<03:50, 16.46s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  79%|███████▉  | 50/63 [13:49<03:33, 16.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  81%|████████  | 51/63 [14:06<03:17, 16.46s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  83%|████████▎ | 52/63 [14:22<03:00, 16.43s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  84%|████████▍ | 53/63 [14:39<02:44, 16.46s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  86%|████████▌ | 54/63 [14:55<02:28, 16.47s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  87%|████████▋ | 55/63 [15:12<02:11, 16.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  89%|████████▉ | 56/63 [15:28<01:55, 16.45s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  90%|█████████ | 57/63 [15:44<01:38, 16.42s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  92%|█████████▏| 58/63 [16:01<01:22, 16.44s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  94%|█████████▎| 59/63 [16:17<01:05, 16.42s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  95%|█████████▌| 60/63 [16:34<00:49, 16.42s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  97%|█████████▋| 61/63 [16:50<00:32, 16.44s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  98%|█████████▊| 62/63 [17:07<00:16, 16.48s/batch]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|██████████| 63/63 [17:15<00:00, 16.44s/batch]\n"
     ]
    }
   ],
   "source": [
    "bert_model.eval()\n",
    "\n",
    "_, _,_ ,predicted_labels = eval_batch(test_dataloader, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "zOSO7yEFkDqO"
   },
   "outputs": [],
   "source": [
    "y_list=y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "JH3gzfBrniYQ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "_2Dca8K8lXYO",
    "outputId": "3c869640-8f7d-4079-dc7f-3f4d4469dd83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[493  14]\n",
      " [ 95 398]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = [1, 0]\n",
    "cm = confusion_matrix(y_list, predicted_labels, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nWhSeRJVR_S"
   },
   "source": [
    "Let's plot confusion matix over the test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "O_ESCQp0VR_S"
   },
   "outputs": [],
   "source": [
    "#from ds_utils.metrics import plot_confusion_matrix\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "#plot_confusion_matrix(y_test, predicted_labels, [1, 0])\n",
    "#labels=['yes', 'no']\n",
    "#plot_confusion_matrix(y_list, predicted_labels, labels)\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2ezDDPgVR_S"
   },
   "source": [
    "# Saving the Fine-Tuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlqlzPdUVR_S",
    "outputId": "70886a5a-82ce-479a-a8ca-41111387c0a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/model_save/vocab.txt',\n",
       " '/content/model_save/special_tokens_map.json',\n",
       " '/content/model_save/added_tokens.json')"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "output_dir = Path(\"__file__\").parents[0].absolute().joinpath(\"model_save\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_to_save = bert_model.module if hasattr(bert_model, 'module') else bert_model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(str(output_dir.absolute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7nlC4eORaBV",
    "outputId": "50afbcc2-1185-4c61-ebf1-df5e29c39f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.838\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "precision = precision_score(y_test, predicted_labels, average='binary')\n",
    "print('Precision: %.3f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tf2MNaEdN7sR",
    "outputId": "67f1a2d8-eba1-41d0-8bd1-dc9f49572bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.972\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_test, predicted_labels, average='binary')\n",
    "print('Recall: %.3f' % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ev-TDAUROMzz",
    "outputId": "8280a831-d638-449a-aae2-e061505f9f0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure: 0.900\n"
     ]
    }
   ],
   "source": [
    "score = f1_score(y_test, predicted_labels, average='binary')\n",
    "print('F-Measure: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRZF6Ye5noeD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_Final (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
